{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjHfWjyC2c1t"
      },
      "source": [
        "### Setup drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m90XYs9cb0Hs",
        "outputId": "98f64771-4cf4-4986-fa3c-fa6fcd9a49c6"
      },
      "outputs": [],
      "source": [
        "# record time\n",
        "import time\n",
        "\n",
        "# start record\n",
        "start = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Tje_Px9Tc23j"
      },
      "outputs": [],
      "source": [
        "# NYC dataset\n",
        "\n",
        "TRAIN_DATASET = \"./train.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oUForsim05Zx"
      },
      "outputs": [],
      "source": [
        "\"\"\" Global Variables \"\"\"\n",
        "\n",
        "MAX_DURATION = 7200  # Maximum trip duration in seconds\n",
        "MIN_DURATION = 180   # Minimum trip duration in seconds\n",
        "MAX_DISTANCE = 60.0  # Maximum distance in kilometers\n",
        "MIN_DISTANCE = 1.5   # Minimum distance in kilometers\n",
        "\n",
        "DAY_OF_WEEK_MAPPING = {\n",
        "    1: 'Sunday',\n",
        "    2: 'Monday',\n",
        "    3: 'Tuesday',\n",
        "    4: 'Wednesday',\n",
        "    5: 'Thursday',\n",
        "    6: 'Friday',\n",
        "    7: 'Saturday'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCOXD0EV1w4M"
      },
      "source": [
        "### Library imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hXcVfUd31yzo"
      },
      "outputs": [],
      "source": [
        "# normal imports\n",
        "import os\n",
        "import requests\n",
        "from IPython.display import Image\n",
        "\n",
        "# for dataframes\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# plots\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "\n",
        "# geodesic\n",
        "from geopy.distance import geodesic\n",
        "\n",
        "# calculation\n",
        "from math import radians, cos, sin, asin, sqrt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzyy7ni9dGCA"
      },
      "source": [
        "### Setup Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "k8OJ8h_PtuXn"
      },
      "outputs": [],
      "source": [
        "# Initialize findspark and import SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WR9cT24Q4K3h"
      },
      "outputs": [],
      "source": [
        "# eda\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf, dayofweek, hour, count, when, year, month, minute, quarter, monotonically_increasing_id\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "\n",
        "# machine learning\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.regression import LinearRegression, GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XeVKsUCmtvVX"
      },
      "outputs": [],
      "source": [
        "# Create a Spark session and context\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Colab\").config('spark.ui.port', '4050').getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQJ5P7hrba33"
      },
      "source": [
        "# ENSF 612 - NYC Taxi Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilKyXZrf1nee"
      },
      "source": [
        "### Read CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "D5lY1DxBD1w7"
      },
      "outputs": [],
      "source": [
        "# # Initialize SparkSession with further optimized settings\n",
        "# spark = SparkSession.builder \\\n",
        "#     .appName(\"Large DataFrame Conversion\") \\\n",
        "#     .config(\"spark.driver.memory\", \"16g\") \\\n",
        "#     .config(\"spark.executor.memory\", \"16g\") \\\n",
        "#     .config(\"spark.executor.memoryOverhead\", \"4g\") \\\n",
        "#     .config(\"spark.driver.cores\", \"4\") \\\n",
        "#     .config(\"spark.executor.cores\", \"4\") \\\n",
        "#     .config(\"spark.network.timeout\", \"12000s\") \\\n",
        "#     .config(\"spark.executor.heartbeatInterval\", \"12000s\") \\\n",
        "#     .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
        "#     .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
        "#     .getOrCreate()\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TaxiDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Define the schema corresponding to the CSV file format\n",
        "schema = StructType([\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"vendor_id\", IntegerType(), True),\n",
        "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
        "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
        "    StructField(\"passenger_count\", IntegerType(), True),\n",
        "    StructField(\"pickup_longitude\", FloatType(), True),\n",
        "    StructField(\"pickup_latitude\", FloatType(), True),\n",
        "    StructField(\"dropoff_longitude\", FloatType(), True),\n",
        "    StructField(\"dropoff_latitude\", FloatType(), True),\n",
        "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
        "    StructField(\"trip_duration\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "taxi_df = spark.read.csv(TRAIN_DATASET, schema=schema, header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR0vpbTQdCCs"
      },
      "source": [
        "## Initial Explaratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZC0jyNLqpEL"
      },
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trip_durations = taxi_df.select(\"trip_duration\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# Now, using Matplotlib to plot the histogram with a log scale\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(trip_durations, log_scale=True, bins=100, kde=False, color=\"red\")\n",
        "\n",
        "plt.title(\"Trip Duration Distribution\")\n",
        "plt.xlabel(\"trip_duration\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.xscale(\"log\") # Set the scale of the x-axis to logarithmic\n",
        "plt.yscale(\"linear\") # The y-scale is linear in the graph provided\n",
        "plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5) # Adding a grid\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5liyPQedr64",
        "outputId": "27cd11d8-14a1-44da-f7d2-9b08eb0f49d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+---------------+----------------+---------------+-----------------+----------------+-------------+\n",
            "|    pickup_datetime|   dropoff_datetime|passenger_count|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|trip_duration|\n",
            "+-------------------+-------------------+---------------+----------------+---------------+-----------------+----------------+-------------+\n",
            "|2016-03-14 17:24:55|2016-03-14 17:32:30|              1|      -73.982155|      40.767937|        -73.96463|       40.765602|          455|\n",
            "|2016-06-12 00:43:35|2016-06-12 00:54:38|              1|      -73.980415|      40.738564|        -73.99948|        40.73115|          663|\n",
            "|2016-01-19 11:35:24|2016-01-19 12:10:48|              1|       -73.97903|       40.76394|        -74.00533|       40.710087|         2124|\n",
            "|2016-04-06 19:32:31|2016-04-06 19:39:40|              1|       -74.01004|       40.71997|        -74.01227|        40.70672|          429|\n",
            "|2016-03-26 13:30:55|2016-03-26 13:38:10|              1|       -73.97305|       40.79321|        -73.97292|        40.78252|          435|\n",
            "+-------------------+-------------------+---------------+----------------+---------------+-----------------+----------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# drop the columns we don't need\n",
        "taxi_df = taxi_df.drop(\"id\", \"vendor_id\", \"store_and_fwd_flag\")\n",
        "taxi_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bTSh5yTiSLD",
        "outputId": "7a1c4f48-5e17-465d-b2a3-1f2d1afcbe9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- pickup_datetime: timestamp (nullable = true)\n",
            " |-- dropoff_datetime: timestamp (nullable = true)\n",
            " |-- passenger_count: integer (nullable = true)\n",
            " |-- pickup_longitude: float (nullable = true)\n",
            " |-- pickup_latitude: float (nullable = true)\n",
            " |-- dropoff_longitude: float (nullable = true)\n",
            " |-- dropoff_latitude: float (nullable = true)\n",
            " |-- trip_duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# print current schema\n",
        "taxi_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MOyWBO0gU-X",
        "outputId": "8f241b93-bf0d-4910-e302-66983cf4ecf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+-------------------+------------------+-------------------+-------------------+-----------------+\n",
            "|summary|   passenger_count|   pickup_longitude|   pickup_latitude|  dropoff_longitude|   dropoff_latitude|    trip_duration|\n",
            "+-------+------------------+-------------------+------------------+-------------------+-------------------+-----------------+\n",
            "|  count|           1458644|            1458644|           1458644|            1458644|            1458644|          1458644|\n",
            "|   mean|1.6645295219395548| -73.97348630489282|40.750920908391734|  -73.9734159469458|   40.7517995149002|959.4922729603659|\n",
            "| stddev| 1.314242167823114|0.07090185842270283| 0.032881186257633|0.07064326809720287|0.03589055560563683|5237.431724497642|\n",
            "|    min|                 0|         -121.93334|         34.359695|        -121.933304|           32.18114|                1|\n",
            "|    max|                 9|          -61.33553|         51.881084|          -61.33553|           43.92103|          3526282|\n",
            "+-------+------------------+-------------------+------------------+-------------------+-------------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# print current summary\n",
        "taxi_df.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12pdszRniZCk",
        "outputId": "72d65c53-a493-47bc-eefa-e200cf60ba1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+----------------+---------------+----------------+---------------+-----------------+----------------+-------------+\n",
            "|pickup_datetime|dropoff_datetime|passenger_count|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|trip_duration|\n",
            "+---------------+----------------+---------------+----------------+---------------+-----------------+----------------+-------------+\n",
            "|              0|               0|              0|               0|              0|                0|               0|            0|\n",
            "+---------------+----------------+---------------+----------------+---------------+-----------------+----------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# check for null values\n",
        "null_counts = taxi_df.select([count(when(col(c).isNull(), c)).alias(c) for c in taxi_df.columns])\n",
        "null_counts.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83aaFyE0f4Ge"
      },
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4GyZUAu0lTpm"
      },
      "outputs": [],
      "source": [
        "def calculate_haversine_distance(pick_up_lat, drop_off_lat, pick_up_lon, drop_off_lon):\n",
        "    # Convert decimal degrees to radians\n",
        "    pick_up_lat, drop_off_lat, pick_up_lon, drop_off_lon = map(radians, [pick_up_lat, drop_off_lat, pick_up_lon, drop_off_lon])\n",
        "\n",
        "    # Haversine formula\n",
        "    dlon = drop_off_lon - pick_up_lon\n",
        "    dlat = drop_off_lat - pick_up_lat\n",
        "    a = sin(dlat / 2)**2 + cos(pick_up_lat) * cos(drop_off_lat) * sin(dlon / 2)**2\n",
        "    c = 2 * asin(sqrt(a))\n",
        "\n",
        "    # Radius of earth in kilometers\n",
        "    earth_radius = 6371\n",
        "    return c * earth_radius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Pt1I8XFGTrIv"
      },
      "outputs": [],
      "source": [
        "def calculate_velocity(distance, duration):\n",
        "    if duration and duration > 0:\n",
        "        # Convert duration from seconds to hours and calculate velocity\n",
        "        return distance / (duration / 3600)\n",
        "    else:\n",
        "        # Return None or a default value if duration is zero or negative\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "y7NMRBIs94PE"
      },
      "outputs": [],
      "source": [
        "# Read the zip code data and create a broadcast variable\n",
        "def read_zipcode_file(filename):\n",
        "    \"\"\"Reads the zip code data from a file and returns a mapping of zip codes to coordinates.\"\"\"\n",
        "    zipcode_mapping = {}\n",
        "\n",
        "    with open(filename, 'r') as file:\n",
        "        for line in file:\n",
        "            zip_code, lat, lon = line.strip().split(', ')\n",
        "            lat_lon = (float(lat), float(lon))\n",
        "            zipcode_mapping[zip_code] = lat_lon\n",
        "\n",
        "    return zipcode_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "e7aosqm9-e99"
      },
      "outputs": [],
      "source": [
        "# Define the UDF\n",
        "def find_nearest_zipcode_udf(lat, lon):\n",
        "    nearest_zip = None\n",
        "    shortest_distance = None\n",
        "\n",
        "    for zip_code, (zip_lat, zip_lon) in broadcast_zipcode_mapping.value.items():\n",
        "        distance = geodesic((lat, lon), (zip_lat, zip_lon)).miles\n",
        "        if shortest_distance is None or distance < shortest_distance:\n",
        "            nearest_zip = zip_code\n",
        "            shortest_distance = distance\n",
        "\n",
        "    return nearest_zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pQZXmpBsEcn",
        "outputId": "59c6d9ac-cb40-4420-b3f7-a467eb2bda9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+---------------+----------------+---------------+-----------------+----------------+-------------+---------+\n",
            "|    pickup_datetime|   dropoff_datetime|passenger_count|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|trip_duration| distance|\n",
            "+-------------------+-------------------+---------------+----------------+---------------+-----------------+----------------+-------------+---------+\n",
            "|2016-03-14 17:24:55|2016-03-14 17:32:30|              1|      -73.982155|      40.767937|        -73.96463|       40.765602|          455|1.4985207|\n",
            "|2016-06-12 00:43:35|2016-06-12 00:54:38|              1|      -73.980415|      40.738564|        -73.99948|        40.73115|          663|1.8055072|\n",
            "|2016-01-19 11:35:24|2016-01-19 12:10:48|              1|       -73.97903|       40.76394|        -74.00533|       40.710087|         2124|6.3850985|\n",
            "|2016-04-06 19:32:31|2016-04-06 19:39:40|              1|       -74.01004|       40.71997|        -74.01227|        40.70672|          429|1.4854984|\n",
            "|2016-03-26 13:30:55|2016-03-26 13:38:10|              1|       -73.97305|       40.79321|        -73.97292|        40.78252|          435|1.1885885|\n",
            "+-------------------+-------------------+---------------+----------------+---------------+-----------------+----------------+-------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Register UDF with FloatType return type\n",
        "haversine_distance_udf = udf(calculate_haversine_distance, FloatType())\n",
        "\n",
        "# Add distance column using the UDF\n",
        "taxi_df_with_distance = taxi_df.withColumn(\n",
        "    \"distance\",\n",
        "    haversine_distance_udf(\n",
        "        col(\"pickup_latitude\"),\n",
        "        col(\"dropoff_latitude\"),\n",
        "        col(\"pickup_longitude\"),\n",
        "        col(\"dropoff_longitude\")\n",
        "    )\n",
        ")\n",
        "\n",
        "taxi_df_with_distance.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4NUmiuiuJ1Z",
        "outputId": "c3ffb963-f0e9-4cc9-f623-384775aa269b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing samples with distance less than 0. Current number of samples = 1458644\n",
            "Current number of samples = 1452747\n"
          ]
        }
      ],
      "source": [
        "print(f\"Removing samples with distance less than 0. Current number of samples = {taxi_df_with_distance.count()}\")\n",
        "taxi_df_with_distance = taxi_df_with_distance.filter(taxi_df_with_distance.distance > 0)\n",
        "print(f\"Current number of samples = {taxi_df_with_distance.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcCSpkoiNk0W",
        "outputId": "54f58435-9566-43bc-8f3a-d0cb1fb8823b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Only keeping samples between 180 seconds and 7200 seconds.\n",
            "Only keeping samples between 1.5 km and 60.0 km.\n",
            "Current number of samples = 961378\n"
          ]
        }
      ],
      "source": [
        "print(f\"Only keeping samples between {MIN_DURATION} seconds and {MAX_DURATION} seconds.\\nOnly keeping samples between {MIN_DISTANCE} km and {MAX_DISTANCE} km.\")\n",
        "taxi_df_with_distance = taxi_df_with_distance.filter((taxi_df_with_distance.trip_duration > MIN_DURATION) & (taxi_df_with_distance.trip_duration < MAX_DURATION) & (taxi_df_with_distance.distance > MIN_DISTANCE) & (taxi_df_with_distance.distance < MAX_DISTANCE))\n",
        "print(f\"Current number of samples = {taxi_df_with_distance.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoGHzPxfTTPh",
        "outputId": "b73c8b84-11fd-4027-a58a-d6bf13535727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+---------------+----------------+---------------+-----------------+----------------+-------------+---------+---------+\n",
            "|    pickup_datetime|   dropoff_datetime|passenger_count|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|trip_duration| distance| velocity|\n",
            "+-------------------+-------------------+---------------+----------------+---------------+-----------------+----------------+-------------+---------+---------+\n",
            "|2016-06-12 00:43:35|2016-06-12 00:54:38|              1|      -73.980415|      40.738564|        -73.99948|        40.73115|          663|1.8055072|9.8036585|\n",
            "|2016-01-19 11:35:24|2016-01-19 12:10:48|              1|       -73.97903|       40.76394|        -74.00533|       40.710087|         2124|6.3850985|10.822201|\n",
            "|2016-05-21 07:54:58|2016-05-21 08:20:49|              1|       -73.96928|       40.79778|        -73.92247|        40.76056|         1551|5.7149806|13.264945|\n",
            "|2016-03-10 21:45:01|2016-03-10 22:05:26|              1|       -73.98105|       40.74434|          -73.973|        40.78999|         1225|5.1211615|15.049944|\n",
            "|2016-05-10 22:08:41|2016-05-10 22:29:55|              1|       -73.98265|       40.76384|        -74.00223|        40.73299|         1274|3.8061395|10.755182|\n",
            "+-------------------+-------------------+---------------+----------------+---------------+-----------------+----------------+-------------+---------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "calculate_velocity_udf = udf(calculate_velocity, FloatType())\n",
        "\n",
        "# Add velocity column using the UDF\n",
        "taxi_df_with_velocity = taxi_df_with_distance.withColumn(\n",
        "    \"velocity\",\n",
        "    calculate_velocity_udf(\"distance\", \"trip_duration\")\n",
        ")\n",
        "\n",
        "taxi_df_with_velocity.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YysCbmoLtyAC",
        "outputId": "a3d48765-d228-4ddd-f62c-736571b98831"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+---------------+----------------+---------------+-----------------+----------------+-------------+---------+---------+--------------+\n",
            "|    pickup_datetime|   dropoff_datetime|passenger_count|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|trip_duration| distance| velocity|severity_scale|\n",
            "+-------------------+-------------------+---------------+----------------+---------------+-----------------+----------------+-------------+---------+---------+--------------+\n",
            "|2016-06-12 00:43:35|2016-06-12 00:54:38|              1|      -73.980415|      40.738564|        -73.99948|        40.73115|          663|1.8055072|9.8036585|             5|\n",
            "|2016-01-19 11:35:24|2016-01-19 12:10:48|              1|       -73.97903|       40.76394|        -74.00533|       40.710087|         2124|6.3850985|10.822201|             5|\n",
            "|2016-05-21 07:54:58|2016-05-21 08:20:49|              1|       -73.96928|       40.79778|        -73.92247|        40.76056|         1551|5.7149806|13.264945|             5|\n",
            "|2016-03-10 21:45:01|2016-03-10 22:05:26|              1|       -73.98105|       40.74434|          -73.973|        40.78999|         1225|5.1211615|15.049944|             5|\n",
            "|2016-05-10 22:08:41|2016-05-10 22:29:55|              1|       -73.98265|       40.76384|        -74.00223|        40.73299|         1274|3.8061395|10.755182|             5|\n",
            "+-------------------+-------------------+---------------+----------------+---------------+-----------------+----------------+-------------+---------+---------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate the quintiles for the 'velocity' column\n",
        "quintiles = taxi_df_with_velocity.approxQuantile(\"velocity\", [0.2, 0.4, 0.6, 0.8], 0.01)\n",
        "\n",
        "# Reverse categorize the velocity into severity scales\n",
        "# Now, lower velocity indicates higher severity (heavier traffic)\n",
        "def reverse_categorize_velocity(value):\n",
        "    for i, threshold in enumerate(reversed(quintiles), start=1):\n",
        "        if value <= threshold:\n",
        "            return 5 - i + 1  # Reverse the scale\n",
        "    return 1  # Lowest severity for highest velocity\n",
        "\n",
        "# Register the function as a UDF (User Defined Function)\n",
        "reverse_categorize_velocity_udf = F.udf(reverse_categorize_velocity, T.IntegerType())\n",
        "\n",
        "# Create a new column in the DataFrame with the reversed severity scale\n",
        "taxi_df_with_reversed_severity = taxi_df_with_velocity.withColumn(\"severity_scale\", reverse_categorize_velocity_udf(F.col(\"velocity\")))\n",
        "\n",
        "# Show the DataFrame with the new severity_scale column\n",
        "taxi_df_with_reversed_severity.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF7H4W21WFqV",
        "outputId": "979d92c5-7897-423f-f481-2dc8aacc15c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+----------------+---------------+-----------------+----------------+-------------+---------+---------+--------------+----+---------------+-----+--------+----+------+\n",
            "|passenger_count|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|trip_duration| distance| velocity|severity_scale|year|quarter_of_year|month|week_day|hour|minute|\n",
            "+---------------+----------------+---------------+-----------------+----------------+-------------+---------+---------+--------------+----+---------------+-----+--------+----+------+\n",
            "|              1|      -73.980415|      40.738564|        -73.99948|        40.73115|          663|1.8055072|9.8036585|             5|2016|              2|    6|       1|   0|    43|\n",
            "|              1|       -73.97903|       40.76394|        -74.00533|       40.710087|         2124|6.3850985|10.822201|             5|2016|              1|    1|       3|  11|    35|\n",
            "|              1|       -73.96928|       40.79778|        -73.92247|        40.76056|         1551|5.7149806|13.264945|             5|2016|              2|    5|       7|   7|    54|\n",
            "|              1|       -73.98105|       40.74434|          -73.973|        40.78999|         1225|5.1211615|15.049944|             5|2016|              1|    3|       5|  21|    45|\n",
            "|              1|       -73.98265|       40.76384|        -74.00223|        40.73299|         1274|3.8061395|10.755182|             5|2016|              2|    5|       3|  22|     8|\n",
            "+---------------+----------------+---------------+-----------------+----------------+-------------+---------+---------+--------------+----+---------------+-----+--------+----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "  Day of the week\n",
        "  ========================\n",
        "  1 = Sunday\n",
        "  2 = Monday\n",
        "  3 = Tuesday\n",
        "  4 = Wednesday\n",
        "  5 = Thursday\n",
        "  6 = Friday\n",
        "  7 = Saturday\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# transform pickup datetime\n",
        "taxi_df_transformed = taxi_df_with_reversed_severity.withColumn(\"year\", year(\"pickup_datetime\")) \\\n",
        "                             .withColumn(\"quarter_of_year\", quarter(\"pickup_datetime\")) \\\n",
        "                             .withColumn(\"month\", month(\"pickup_datetime\")) \\\n",
        "                             .withColumn(\"week_day\", dayofweek(\"pickup_datetime\")) \\\n",
        "                             .withColumn(\"hour\", hour(\"pickup_datetime\")) \\\n",
        "                             .withColumn(\"minute\", minute(\"pickup_datetime\"))\n",
        "\n",
        "# drop pickup and dropoff datetime\n",
        "taxi_df_week = taxi_df_transformed.drop(\"pickup_datetime\", \"dropoff_datetime\")\n",
        "taxi_df_week.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "QNNr0239hZY-"
      },
      "outputs": [],
      "source": [
        "# create zip code mapping\n",
        "all_coordinates_text = \"./all_coordinates.txt\"\n",
        "zipcode_mapping = read_zipcode_file(all_coordinates_text)\n",
        "broadcast_zipcode_mapping = spark.sparkContext.broadcast(zipcode_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "EtNt3MQOAGPu"
      },
      "outputs": [],
      "source": [
        "# Register UDF\n",
        "nearest_zipcode_udf = udf(find_nearest_zipcode_udf, StringType())\n",
        "\n",
        "# Apply UDF on DataFrame\n",
        "taxi_df_with_zip_code = taxi_df_week.withColumn(\"pickup\", nearest_zipcode_udf(F.col(\"pickup_latitude\"), F.col(\"pickup_longitude\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "taxi_df_with_zip_code = taxi_df_with_zip_code.withColumn(\"dropoff\", nearest_zipcode_udf(F.col(\"dropoff_latitude\"), F.col(\"dropoff_longitude\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "2A14vqiBnW-g",
        "outputId": "dd8dc8a2-4f48-4219-9839-20fd03d0a782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+---------+---------+--------------+----+---------------+-----+--------+----+------+------+-------+\n",
            "|trip_duration| distance| velocity|severity_scale|year|quarter_of_year|month|week_day|hour|minute|pickup|dropoff|\n",
            "+-------------+---------+---------+--------------+----+---------------+-----+--------+----+------+------+-------+\n",
            "|          663|1.8055072|9.8036585|             5|2016|              2|    6|       1|   0|    43| 10010|  10012|\n",
            "|         2124|6.3850985|10.822201|             5|2016|              1|    1|       3|  11|    35| 10103|  10038|\n",
            "|         1551|5.7149806|13.264945|             5|2016|              2|    5|       7|   7|    54| 10025|  11106|\n",
            "|         1225|5.1211615|15.049944|             5|2016|              1|    3|       5|  21|    45| 10016|  10024|\n",
            "|         1274|3.8061395|10.755182|             5|2016|              2|    5|       3|  22|     8| 10019|  10014|\n",
            "|         1128|3.7730958|12.041796|             5|2016|              2|    5|       1|  11|    16| 10119|  10021|\n",
            "|         1114| 1.859483|6.0091014|             5|2016|              1|    2|       6|   9|    52| 10022|  10112|\n",
            "|         1414| 6.382836|16.250502|             5|2016|              2|    5|       6|   0|    43| 10012|  10023|\n",
            "|         2316| 3.428086| 5.328631|             5|2016|              2|    4|       2|  17|    29| 10018|  10012|\n",
            "|          731|2.5386717|12.502351|             5|2016|              2|    4|       5|   8|    48| 10119|  10012|\n",
            "|         1317|4.6052012|12.588249|             5|2016|              2|    6|       2|   9|    55| 10278|  10168|\n",
            "|          486|2.5059261|18.562416|             5|2016|              1|    2|       1|   2|    23| 10016|  10065|\n",
            "|          652|1.7245501| 9.522056|             5|2016|              2|    4|       6|  12|    12| 10173|  10065|\n",
            "|          423|2.0670848| 17.59221|             5|2016|              2|    4|       7|   3|    34| 10018|  10011|\n",
            "|         1163|4.8747926|15.089642|             5|2016|              2|    6|       7|  10|    36| 10119|  10005|\n",
            "|         2485|20.602575|29.846788|             1|2016|              2|    6|       6|   8|    15| 10028|  11430|\n",
            "|         1283| 4.559525|12.793679|             5|2016|              1|    2|       1|  13|    27| 10075|  10009|\n",
            "|         1130|6.0561094|  19.2938|             5|2016|              1|    2|       7|  21|    56| 10165|  10026|\n",
            "|          694| 3.738742| 19.39405|             5|2016|              2|    6|       2|  23|     7| 11211|  11216|\n",
            "|          892|2.5248492|10.189974|             5|2016|              2|    6|       2|  21|    57| 10002|  11201|\n",
            "+-------------+---------+---------+--------------+----+---------------+-----+--------+----+------+------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "taxi_df_final = taxi_df_with_zip_code.drop(\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\", \"passenger_count\")\n",
        "taxi_df_final.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "961378"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "taxi_df_final.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "pickup_pd = taxi_df_final.select(\"pickup\").toPandas()\n",
        "\n",
        "# Plotting the histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(pickup_pd['pickup'], bins=50, color='blue', edgecolor='black')\n",
        "plt.title('Histogram of Pickup')\n",
        "plt.xlabel('Pickup')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\SPARK\\python\\pyspark\\sql\\pandas\\conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
            "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
            "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
            "  warn(msg)\n"
          ]
        },
        {
          "ename": "PythonException",
          "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py\", line 708, in readinto\n    raise\nTimeoutError: timed out\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[1;32md:\\chris\\Documents\\UofC\\MEng Soft\\fall\\ENSF 607\\MEng-Fall-Group\\ENSF 612\\staging.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/chris/Documents/UofC/MEng%20Soft/fall/ENSF%20607/MEng-Fall-Group/ENSF%20612/staging.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pandas_df \u001b[39m=\u001b[39m taxi_df_final\u001b[39m.\u001b[39;49mtoPandas()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/chris/Documents/UofC/MEng%20Soft/fall/ENSF%20607/MEng-Fall-Group/ENSF%20612/staging.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m output_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain_cleaned.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/chris/Documents/UofC/MEng%20Soft/fall/ENSF%20607/MEng-Fall-Group/ENSF%20612/staging.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pandas_df\u001b[39m.\u001b[39mto_csv(output_path, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
            "File \u001b[1;32mC:\\SPARK\\python\\pyspark\\sql\\pandas\\conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m rows \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect()\n\u001b[0;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(rows) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    204\u001b[0m     pdf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_records(\n\u001b[0;32m    205\u001b[0m         rows, index\u001b[39m=\u001b[39m\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(rows)), columns\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     )\n",
            "File \u001b[1;32mC:\\SPARK\\python\\pyspark\\sql\\dataframe.py:1257\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m \n\u001b[0;32m   1239\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[39m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1256\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc):\n\u001b[1;32m-> 1257\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcollectToPython()\n\u001b[0;32m   1258\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
            "File \u001b[1;32mC:\\SPARK\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "File \u001b[1;32mC:\\SPARK\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py\", line 708, in readinto\n    raise\nTimeoutError: timed out\n"
          ]
        }
      ],
      "source": [
        "pandas_df = taxi_df_final.toPandas()\n",
        "output_path = \"train_cleaned.csv\"\n",
        "pandas_df.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "po-TacABANd-"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o226.csv.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32md:\\chris\\Documents\\UofC\\MEng Soft\\fall\\ENSF 607\\MEng-Fall-Group\\ENSF 612\\staging.ipynb Cell 36\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/chris/Documents/UofC/MEng%20Soft/fall/ENSF%20607/MEng-Fall-Group/ENSF%20612/staging.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m save_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./train_cleaned.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/chris/Documents/UofC/MEng%20Soft/fall/ENSF%20607/MEng-Fall-Group/ENSF%20612/staging.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Repartition to a single partition if the dataset is not too large\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/chris/Documents/UofC/MEng%20Soft/fall/ENSF%20607/MEng-Fall-Group/ENSF%20612/staging.ipynb#X50sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m taxi_df_final\u001b[39m.\u001b[39;49mlimit(\u001b[39m10\u001b[39;49m)\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mmode(\u001b[39m\"\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mcsv(save_path)\n",
            "File \u001b[1;32mC:\\SPARK\\python\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[39m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[39m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[39m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mcsv(path)\n",
            "File \u001b[1;32mC:\\SPARK\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "File \u001b[1;32mC:\\SPARK\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32mC:\\SPARK\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o226.csv.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n"
          ]
        }
      ],
      "source": [
        "# Stop the SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJpeRQRb5y4T"
      },
      "source": [
        "### End of notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6-iqDYI52VH"
      },
      "outputs": [],
      "source": [
        "end = time.time()\n",
        "delta_time = end - start\n",
        "print(F\"It took {delta_time} s to run the notebook.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
